1
00:00:00,016 --> 00:00:08,895
♪ (upbeat music) ♪

2
00:00:08,895 --> 00:00:10,824
Good morning everyone, I'm Alina,

3
00:00:10,824 --> 00:00:14,492
program manager for TensorFlow.

4
00:00:14,492 --> 00:00:15,493
(applause)

5
00:00:15,493 --> 00:00:17,461
Thank you.

6
00:00:17,461 --> 00:00:20,929
Welcome to the 2019
TensorFlow Developer Summit.

7
00:00:20,929 --> 00:00:24,334
This is our third annual and
largest developer summit to date,

8
00:00:24,334 --> 00:00:26,339
and I'm so happy to have
all of you here

9
00:00:26,339 --> 00:00:28,601
both right here in the room
and on the Livestream.

10
00:00:28,601 --> 00:00:29,624
Welcome.

11
00:00:29,624 --> 00:00:31,431
So I'm just curious
by a show of hands,

12
00:00:31,431 --> 00:00:35,096
who traveled a little bit further,
maybe, to get here?

13
00:00:35,096 --> 00:00:37,667
Europe?

14
00:00:37,667 --> 00:00:39,788
Asia?

15
00:00:39,788 --> 00:00:41,309
Africa?

16
00:00:41,309 --> 00:00:43,635
As far as Australia?

17
00:00:43,635 --> 00:00:45,206
Woo, awesome.

18
00:00:45,206 --> 00:00:48,297
Welcome to all of you.

19
00:00:48,297 --> 00:00:52,367
We have a lot of great talks ahead,
some exciting announcements

20
00:00:52,367 --> 00:00:56,089
and cool demos, so let's get going.

21
00:00:56,089 --> 00:00:59,350
We are living in a formative moment
of history right now,

22
00:00:59,350 --> 00:01:03,079
where machine learning is experiencing
an unprecedented revolution.

23
00:01:03,079 --> 00:01:07,215
The way we fundamentally think about
and interact with computer systems

24
00:01:07,215 --> 00:01:08,938
has inherently changed

25
00:01:08,938 --> 00:01:11,795
due to the breakthroughs
in the field of AI.

26
00:01:11,795 --> 00:01:15,366
and this is due to three major factors.

27
00:01:15,366 --> 00:01:20,866
First, we have lots more compute
specially designed ML accelerator

28
00:01:20,866 --> 00:01:22,647
like these TPUs,

29
00:01:22,647 --> 00:01:28,439
let you train models faster
than ever before.

30
00:01:28,439 --> 00:01:32,678
Secondly, we have breakthroughs
in the field of machine learning.

31
00:01:32,678 --> 00:01:34,989
Their novel algorithms
created every month

32
00:01:34,989 --> 00:01:39,463
like a BERT and an innovative approach
to natural language processing

33
00:01:39,463 --> 00:01:41,145
which lets anyone around the world

34
00:01:41,145 --> 00:01:46,461
train their own state-of-the-art
question-answering system.

35
00:01:46,461 --> 00:01:49,991
And finally, we have lots
and lots of data.

36
00:01:49,991 --> 00:01:54,781
We're seeing new waves of data sets
come from all kinds of disciplines.

37
00:01:54,781 --> 00:01:59,367
For example, the new
open images extended data set.

38
00:01:59,367 --> 00:02:04,905
This is a collection
of over 478,000 images

39
00:02:04,905 --> 00:02:08,430
that volunteers have have added

40
00:02:08,430 --> 00:02:12,992
with the pursuit
of inclusivity and diversity.

41
00:02:12,992 --> 00:02:16,303
So, all three of these
are basically changing

42
00:02:16,303 --> 00:02:19,577
how we solve challenging,
real-world problems,

43
00:02:19,577 --> 00:02:23,263
and it's really cool to see
that TensorFlow is the platform

44
00:02:23,263 --> 00:02:27,390
that's powering
this machine learning revolution.

45
00:02:27,390 --> 00:02:31,890
It's allowing developers, businesses
and researchers around the world

46
00:02:31,890 --> 00:02:34,565
to benefit from intelligent applications.

47
00:02:34,565 --> 00:02:35,568
And we've been really amazed

48
00:02:35,568 --> 00:02:40,434
by what the community
has built with TensorFlow.

49
00:02:40,434 --> 00:02:43,799
Developers have been
using TensorFlow to solve problems

50
00:02:43,799 --> 00:02:46,881
in in their local communities.

51
00:02:46,881 --> 00:02:49,778
So I don't know if any of you
were in the Bay Area

52
00:02:49,778 --> 00:02:51,938
during the tragic Paradise fire,

53
00:02:51,938 --> 00:02:53,518
but one of the consequences was

54
00:02:53,518 --> 00:02:55,828
that air quality was really bad.

55
00:02:55,828 --> 00:03:02,153
It was in the high to mid to 200s
on the Air Quality Index.

56
00:03:02,153 --> 00:03:08,634
And as difficult as this was for
us in Delhi, India during winter.

57
00:03:08,634 --> 00:03:15,277
The air quality can get up to about 
the 400s on Air Quality Index,

58
00:03:15,277 --> 00:03:17,587
and this is considered very dangerous.

59
00:03:17,587 --> 00:03:20,840
So pollution sensors
can help gauge air quality

60
00:03:20,840 --> 00:03:24,284
but they're very expensive
to deploy at scale.

61
00:03:24,284 --> 00:03:29,522
So a group of students in Delhi
built image classifiers in TensorFlow

62
00:03:29,522 --> 00:03:35,094
and use those to build an app
called Air Cognizer,

63
00:03:35,094 --> 00:03:40,678
and what it does is basically just
by using the images on a smartphone.

64
00:03:40,678 --> 00:03:46,736
It gives an accurate
estimation of the air quality.

65
00:03:46,736 --> 00:03:48,623
Businesses are also fundamentally

66
00:03:48,623 --> 00:03:51,934
improving their products and
services built with TensorFlow,

67
00:03:51,934 --> 00:03:56,381
for example, where it strives
to keep its global users informed

68
00:03:56,381 --> 00:03:59,151
with relevant and healthy content.

69
00:03:59,151 --> 00:04:00,294
But this can be hard,

70
00:04:00,294 --> 00:04:03,732
when the users follow hundreds
or even thousands of people,

71
00:04:03,732 --> 00:04:08,052
so to solve this, Twitter launched
ranked timeline, an ML power feed

72
00:04:08,052 --> 00:04:11,067
which has the most relevant tweets
at the top of the time timeline,

73
00:04:11,067 --> 00:04:14,116
ensuring users never missed their
best and most relevant content.

74
00:04:14,116 --> 00:04:16,668
And by using TensorFlow's
ecosystem of tools

75
00:04:16,668 --> 00:04:19,081
like TensorFlow Hub, TensorBoard

76
00:04:19,081 --> 00:04:21,279
and TensorFlow Model Analysis,

77
00:04:21,279 --> 00:04:26,500
Twitter was able to reduce
both training and model iteration time

78
00:04:26,500 --> 00:04:32,350
as well as increase the timeline
quality and engagement for users.

79
00:04:32,350 --> 00:04:36,877
Specific industries are also
being very much transformed by ML.

80
00:04:36,877 --> 00:04:38,592
G Healthcare, for example

81
00:04:38,592 --> 00:04:41,282
is using TensorFlow
to improve MRI imaging.

82
00:04:41,282 --> 00:04:45,832
These TensorFlow models,
they're real-time on MRI scanners

83
00:04:45,832 --> 00:04:50,110
and can actually detect the orientation
of the patient inside the scanner.

84
00:04:50,110 --> 00:04:52,288
And this is really great

85
00:04:52,288 --> 00:04:55,356
because not only does this
help the diagnosis,

86
00:04:55,356 --> 00:05:00,247
but also lowers the errors
and exam time.

87
00:05:00,247 --> 00:05:05,936
But also, what's really cool is
it basically expands this technology

88
00:05:05,936 --> 00:05:10,930
to many many more people around the world.

89
00:05:10,930 --> 00:05:13,413
TensorFlow also powers
bleeding-edge research.

90
00:05:13,413 --> 00:05:16,837
A team of scientists, researchers
and engineers

91
00:05:16,837 --> 00:05:20,425
at nurse Oak Ridge National Laboratory
at VIDYA

92
00:05:20,425 --> 00:05:22,519
recently won the Gordon Bell Prize

93
00:05:22,519 --> 00:05:24,693
for applying deeplearning

94
00:05:24,693 --> 00:05:29,088
to study the effects
of extreme weather patterns

95
00:05:29,088 --> 00:05:31,890
using high-performance computing.

96
00:05:31,890 --> 00:05:36,635
They built and scaled a neural
network using TensorFlow,

97
00:05:36,635 --> 00:05:40,858
of course, to a run on Summit,
the world's fastest supercomputer.

98
00:05:40,858 --> 00:05:42,609
They achieved a peak
and sustained throughput

99
00:05:42,609 --> 00:05:45,887
of 1.13 exaFLOPS and FPC-16

100
00:05:45,887 --> 00:05:49,598
which is equivalent to more than
a quantalian computations per second.

101
00:05:49,598 --> 00:05:53,451
I think I need to pause for a second
because that is ridiculously fast.

102
00:05:53,451 --> 00:05:56,933
Right?

103
00:05:56,933 --> 00:05:59,227
In addition to these awesome examples,

104
00:05:59,227 --> 00:06:02,008
there are thousands and thousands
of people all over the world

105
00:06:02,008 --> 00:06:04,509
doing amazing work using TensorFlow,

106
00:06:04,509 --> 00:06:07,470
and the power and impact
of TensorFlow would not be what it is

107
00:06:07,470 --> 00:06:10,106
without all of you, thank you.

108
00:06:10,106 --> 00:06:13,281
It's with your help and interest

109
00:06:13,281 --> 00:06:17,658
that TensorFlow has become the most
widely adopted ML framework in the world.

110
00:06:17,658 --> 00:06:21,666
And right here, I'd like to
show the latest map of GitHub stars

111
00:06:21,666 --> 00:06:24,590
who self-identified their location.

112
00:06:24,590 --> 00:06:26,916
I'm sure many of the dots
on this map are right here

113
00:06:26,916 --> 00:06:28,639
in the room and on the Livestream,

114
00:06:28,639 --> 00:06:33,108
so I just want to say
thank you one more time.

115
00:06:33,108 --> 00:06:35,125
And this growth
has been absolutely amazing.

116
00:06:35,125 --> 00:06:39,761
TensorFlow has been downloaded
over 41 million times,

117
00:06:39,761 --> 00:06:45,214
and has over 1800 contributors worldwide.

118
00:06:45,214 --> 00:06:48,611
Last November, we celebrated
TensorFlow's third birthday

119
00:06:48,611 --> 00:06:51,133
by taking a look back
at the different components

120
00:06:51,133 --> 00:06:53,758
that we've added throughout the years.

121
00:06:53,758 --> 00:06:55,190
But today, we'd like to talk

122
00:06:55,190 --> 00:06:57,594
about how TensorFlow
has matured as a platform

123
00:06:57,594 --> 00:07:00,180
to become an entire
end-to-end ecosystem.

124
00:07:00,180 --> 00:07:03,585
And TensorFlow 2.0 is
the start of a new era,

125
00:07:03,585 --> 00:07:05,702
and we're committed 
and focused on making it

126
00:07:05,702 --> 00:07:08,970
the best ML platform for all our users.

127
00:07:08,970 --> 00:07:11,052
To talk more about TensorFlow 2.0

128
00:07:11,052 --> 00:07:12,729
I'd like to introduce Rajat Monga,

129
00:07:12,729 --> 00:07:16,657
Engineering Director
of TensorFlow on stage.

130
00:07:16,657 --> 00:07:17,658
Thank you.

131
00:07:17,658 --> 00:07:22,276
(applause)

132
00:07:22,276 --> 00:07:24,518
Thank You, Alina.

133
00:07:24,518 --> 00:07:26,645
Hello, everyone, I'm Rajat.

134
00:07:26,645 --> 00:07:28,170
I am an engineer at TensorFlow

135
00:07:28,170 --> 00:07:31,233
and have been involved
with this since the very beginning.

136
00:07:31,233 --> 00:07:34,123
It's been great to see
what we've been up to

137
00:07:34,123 --> 00:07:35,401
over the last few years.

138
00:07:35,401 --> 00:07:36,465
All the amazing growth

139
00:07:36,465 --> 00:07:40,963
and all the amazing things
that you've done with it.

140
00:07:40,963 --> 00:07:43,377
It's also been great to hear from you.

141
00:07:43,377 --> 00:07:45,477
You told us what you like about TensorFlow

142
00:07:45,477 --> 00:07:47,747
and equally importantly,
what you would like to see improved

143
00:07:47,747 --> 00:07:48,858
in TensorFlow.

144
00:07:48,858 --> 00:07:51,800
Your feedback has been loud and clear.

145
00:07:51,800 --> 00:07:56,184
You asked for simpler, more intuitive
APIs in developer experiences.

146
00:07:56,184 --> 00:07:59,436
You pointed out areas
of redundancy and complexity,

147
00:07:59,436 --> 00:08:03,993
and you asked for better
documentation and examples,

148
00:08:03,993 --> 00:08:08,194
and this is exactly what we've been
focusing on with TensorFlow 2.0.

149
00:08:08,194 --> 00:08:10,219
To make it easy, we focused on Keras

150
00:08:10,219 --> 00:08:12,105
for a single set of API's

151
00:08:12,105 --> 00:08:16,009
and combine it with Eager Execution
for the simplicity of Python.

152
00:08:16,009 --> 00:08:18,858
With flexibility to try the craziest ideas

153
00:08:18,858 --> 00:08:21,025
and ability to go beyond an exaFLOP

154
00:08:21,025 --> 00:08:23,785
TensorFlow is more powerful than ever.

155
00:08:23,785 --> 00:08:26,339
With the same robustness
and performance

156
00:08:26,339 --> 00:08:31,793
you expect in production,
battle-tested in Google.

157
00:08:31,793 --> 00:08:34,765
Let's start with the overall
architecture for TensorFlow.

158
00:08:34,765 --> 00:08:37,445
You may be familiar
with this high-level architecture.

159
00:08:37,445 --> 00:08:39,563
There have been lots
of components and features

160
00:08:39,563 --> 00:08:41,422
we've added throughout the years

161
00:08:41,422 --> 00:08:45,231
to help support workloads
to go from training to deployment.

162
00:08:45,231 --> 00:08:47,467
With TensorFlow 2.0,
we're really making sure

163
00:08:47,467 --> 00:08:52,031
that these components
work better together.

164
00:08:52,031 --> 00:08:54,889
Here's how these powerful
API components fit together

165
00:08:54,889 --> 00:08:56,881
for the entire training workflow.

166
00:08:56,881 --> 00:09:00,210
With tf.data for data
ingestion and transformation,

167
00:09:00,210 --> 00:09:03,682
keras and premade estimators
from model building,

168
00:09:03,682 --> 00:09:06,011
training with eager execution and graphs,

169
00:09:06,011 --> 00:09:12,298
and finally packaging
for deployment with SavedModel.

170
00:09:12,298 --> 00:09:15,163
Let's take a look at some examples.

171
00:09:15,163 --> 00:09:16,959
The first thing you need is data.

172
00:09:16,959 --> 00:09:19,944
Often, you may want
to validate results or test your new ideas

173
00:09:19,944 --> 00:09:22,793
in common public data set.

174
00:09:22,793 --> 00:09:24,087
TensorFlow data sets includes

175
00:09:24,087 --> 00:09:27,150
a large and rapidly growing
collection of public data sets

176
00:09:27,150 --> 00:09:29,751
that you can get started with very easily.

177
00:09:29,751 --> 00:09:30,988
And combined with tf.data,

178
00:09:30,988 --> 00:09:35,525
it is simple to wrap your own data too.

179
00:09:35,525 --> 00:09:38,034
Here is a small sample
of the data sets that are available,

180
00:09:38,034 --> 00:09:44,277
and all of these
and many more are included there.

181
00:09:44,277 --> 00:09:47,357
Then with keras,
you can express the model with layers,

182
00:09:47,357 --> 00:09:50,064
just as you are used to thinking about it.

183
00:09:50,064 --> 00:09:52,931
Standard training and evaluation
is packaged as well,

184
00:09:52,931 --> 00:09:58,931
with model.fit and .evaluate.

185
00:09:58,931 --> 00:10:02,284
Since deep learning models
are often computationally expensive,

186
00:10:02,284 --> 00:10:05,748
you may want to try scaling this
across more than one device.

187
00:10:05,748 --> 00:10:07,931
TensorFlow comes pre-built
with MirroredStrategy

188
00:10:07,931 --> 00:10:14,861
that works with small additions 
to your code.

189
00:10:14,861 --> 00:10:17,422
Starting from a pre-trained model 
or component

190
00:10:17,422 --> 00:10:21,105
also works well to reduce some
of this computational cost.

191
00:10:21,105 --> 00:10:22,248
To make it easy,

192
00:10:22,248 --> 00:10:25,558
TensorFlow Hub provides a large
collection of pre-trained components

193
00:10:25,558 --> 00:10:27,136
that you can include in your model

194
00:10:27,136 --> 00:10:33,451
and even fine tune
for your specific data set.

195
00:10:33,451 --> 00:10:36,769
Keras and .estimator
offers high-level building blocks

196
00:10:36,769 --> 00:10:38,706
for an easy-to-use package.

197
00:10:38,706 --> 00:10:43,208
They come with everything
you might need for typical training jobs.

198
00:10:43,208 --> 00:10:45,704
But, sometimes you
need a bit more control.

199
00:10:45,704 --> 00:10:51,695
For example, when you're
exploring new kinds of algorithms.

200
00:10:51,695 --> 00:10:55,567
Let's say, you wanted to build
a custom encoder for machine translation.

201
00:10:55,567 --> 00:10:58,406
Here's how you might do this
by subclassing a model.

202
00:10:58,406 --> 00:10:59,407
Here, you can focus

203
00:10:59,407 --> 00:11:01,812
on implementing
the computational algorithm,

204
00:11:01,812 --> 00:11:05,437
and let the framework
take care of the rest.

205
00:11:05,437 --> 00:11:07,207
And you could even
customize the training loop

206
00:11:07,207 --> 00:11:15,689
to get full control over the gradients
and the optimization process.

207
00:11:15,689 --> 00:11:16,896
While training models,

208
00:11:16,896 --> 00:11:19,523
whether packaged with keras
or more complex ones,

209
00:11:19,523 --> 00:11:22,257
it's often valuable to
understand the progress,

210
00:11:22,257 --> 00:11:25,145
and even analyze the model in detail.

211
00:11:25,145 --> 00:11:28,964
TensorBoard provides
a lot of visualization to help with this,

212
00:11:28,964 --> 00:11:31,070
and now it comes full integration

213
00:11:31,070 --> 00:11:33,616
with intercollab
and other Jupiter notebooks,

214
00:11:33,616 --> 00:11:35,753
allowing you to see
the same visualizations

215
00:11:35,753 --> 00:11:40,271
right from within your notebook.

216
00:11:40,271 --> 00:11:44,904
All of these features
are available in TensorFlow 2.0,

217
00:11:44,904 --> 00:11:47,619
and I'm really excited to announce
that our alpha release is available

218
00:11:47,619 --> 00:11:50,032
for you as of today.

219
00:11:50,032 --> 00:11:56,415
(applause)

220
00:11:56,415 --> 00:11:58,511
Many of you in the room
and across the world

221
00:11:58,511 --> 00:12:02,510
really helped with lots of work
in testing to make this possible.

222
00:12:02,510 --> 00:12:05,281
I really like to take this moment 
to thank you all.

223
00:12:05,281 --> 00:12:07,406
Please give yourself a round of applause.

224
00:12:07,406 --> 00:12:09,083
We really couldn't
have done this without you.

225
00:12:09,083 --> 00:12:17,151
(applause)

226
00:12:17,151 --> 00:12:19,921
In addition to all the great
improvements we talked about,

227
00:12:19,921 --> 00:12:21,558
this release comes
with a Conversion Script

228
00:12:21,558 --> 00:12:23,599
to help you upgrade from 1.X.,

229
00:12:23,599 --> 00:12:27,121
and a compatibility module
to give you access to 1.X. APIs

230
00:12:27,121 --> 00:12:30,075
for easy transition,

231
00:12:30,075 --> 00:12:36,861
and we are working towards the full release
over the next quarter.

232
00:12:36,861 --> 00:12:39,616
There's a lot of work going
on to make TensorFlow 2.0

233
00:12:39,616 --> 00:12:41,263
really work well for you.

234
00:12:41,263 --> 00:12:43,500
You can track the progress
and provide feedback

235
00:12:43,500 --> 00:12:51,109
on the TensorFlow GitHub projects page.

236
00:12:51,109 --> 00:12:52,768
You asked for better documentation

237
00:12:52,768 --> 00:13:00,939
and we worked to streamline our docks
for APIs, guides and tutorials.

238
00:13:00,939 --> 00:13:02,838
All of this material
will be available today

239
00:13:02,838 --> 00:13:05,726
on the newly redesigned
TensorFlow.org website.

240
00:13:05,726 --> 00:13:10,412
Where you'll find more examples,
documentation and tools to get started.

241
00:13:10,412 --> 00:13:11,459
We're really very excited

242
00:13:11,459 --> 00:13:14,697
about these changes and what's to come.

243
00:13:14,697 --> 00:13:16,816
And to tell you more about
improvements in TensorFlow

244
00:13:16,816 --> 00:13:18,045
for research and production.

245
00:13:18,045 --> 00:13:20,960
I'd like to welcome Megan Kacholia
on stage, thank you.

246
00:13:20,960 --> 00:13:28,023
(applause)

247
00:13:28,023 --> 00:13:29,158
Thanks, Rajat.

248
00:13:29,158 --> 00:13:32,425
TensorFlow has always been a platform
for research to production.

249
00:13:32,425 --> 00:13:34,679
We just saw how TensorFlow,
as high-level APIs,

250
00:13:34,679 --> 00:13:37,306
make it easy to get started
and build your models.

251
00:13:37,306 --> 00:13:40,999
Now, let's talk about how it improves
powerful experimentation for researchers,

252
00:13:40,999 --> 00:13:43,235
and let's you take models
from research and prototyping

253
00:13:43,235 --> 00:13:45,876
all the way through to production.

254
00:13:45,876 --> 00:13:48,972
Researchers have been using TensorFlow
for state-of-the-art research.

255
00:13:48,972 --> 00:13:50,835
We can see it in paper publications,

256
00:13:50,835 --> 00:13:54,636
which are shown over the past few years 
in this chart.

257
00:13:54,636 --> 00:13:58,690
But powerful experimentation begins
and really needs flexibility.

258
00:13:58,690 --> 00:14:01,637
This begins with Eager Execution
with TensorFlow.

259
00:14:01,637 --> 00:14:06,334
In TensorFlow 2.0 by default, every Python
command is immediately executed.

260
00:14:06,334 --> 00:14:09,009
This means you can write your code
in the style you're used to

261
00:14:09,009 --> 00:14:10,968
without having to use [such and run.]

262
00:14:10,968 --> 00:14:14,668
This also makes a big difference
in the realm of debugging.

263
00:14:14,668 --> 00:14:16,137
As you iterate through with Eager Mode,

264
00:14:16,137 --> 00:14:19,447
you'll eventually want to distribute
your code onto GPUs, TPUs

265
00:14:19,447 --> 00:14:21,514
and other hardware or accelerators.

266
00:14:21,514 --> 00:14:23,341
For this, we've
provided tf.function

267
00:14:23,341 --> 00:14:26,858
which turns your eager code
into a graph, function by function.

268
00:14:26,858 --> 00:14:28,351
You get all of the familiar tools

269
00:14:28,351 --> 00:14:31,208
like Python, control-flow, asserts,
even print

270
00:14:31,208 --> 00:14:34,803
but can convert to a graph 
anytime you need to,

271
00:14:34,803 --> 00:14:37,817
including when you're ready
to move your model into production.

272
00:14:37,817 --> 00:14:41,232
And even with this,
you'll continue to get great debugging.

273
00:14:41,232 --> 00:14:43,711
Debug ability is great,
not just in Eager,

274
00:14:43,711 --> 00:14:46,815
but we've made huge improvements
in tf.function and graphs as well.

275
00:14:46,815 --> 00:14:48,323
In this example shown here,

276
00:14:48,323 --> 00:14:51,634
we're splitting a tensor
using tf.function which creates a graph,

277
00:14:51,634 --> 00:14:54,478
but because of the mismatched inputs,
you get an error.

278
00:14:54,478 --> 00:14:56,698
As you can see,
we now give users the information

279
00:14:56,698 --> 00:14:59,727
about the file and the line number
where the error occurred in the model

280
00:14:59,727 --> 00:15:01,329
to help you more quickly
track things down,

281
00:15:01,329 --> 00:15:03,307
so you can continue iterating.

282
00:15:03,307 --> 00:15:07,587
We've made the error messages concise,
easy to understand and actionable.

283
00:15:07,587 --> 00:15:10,289
We hope you enjoy these changes
and they make it much easier for you

284
00:15:10,289 --> 00:15:14,336
to quickly iterate and progress 
with your models.

285
00:15:14,336 --> 00:15:16,557
Performance is another area 
we know that researchers

286
00:15:16,557 --> 00:15:19,576
as well as all users
for that matter, care about,

287
00:15:19,576 --> 00:15:22,553
and we've continued improving
core performance in TensorFlow.

288
00:15:22,553 --> 00:15:23,644
Since last year,

289
00:15:23,644 --> 00:15:28,251
we've sped up training on eight
NVIDIA Tesla V100 by almost double.

290
00:15:28,251 --> 00:15:30,505
Using a Google Cloud TPU V2,

291
00:15:30,505 --> 00:15:33,092
we've boosted performance by 1.6x.

292
00:15:33,092 --> 00:15:35,103
And with Intel MKL Acceleration

293
00:15:35,103 --> 00:15:38,127
we've got an inference speed up
by almost three times.

294
00:15:38,127 --> 00:15:41,334
Performance will continue to be
a big focus of TensorFlow 2.0

295
00:15:41,334 --> 00:15:45,329
and a core part of our
progress to final release.

296
00:15:45,329 --> 00:15:48,655
TensorFlow also provides
flexibility to enable researchers,

297
00:15:48,655 --> 00:15:50,313
and this is with many add-on libraries

298
00:15:50,313 --> 00:15:52,480
that extend and expand TensorFlow

299
00:15:52,480 --> 00:15:54,273
in new and useful ways.

300
00:15:54,273 --> 00:15:56,129
Some of these add-on libraries
or extensions

301
00:15:56,129 --> 00:15:57,469
to make certain problems easier,

302
00:15:57,469 --> 00:15:59,406
like TF.Text with Unicode

303
00:15:59,406 --> 00:16:01,416
and the new ragged Tensor type.

304
00:16:01,416 --> 00:16:03,203
In other cases, it lets us explore

305
00:16:03,203 --> 00:16:06,259
how we can make machine
learning models fairer and safer

306
00:16:06,259 --> 00:16:08,039
by a TF Privacy.

307
00:16:08,039 --> 00:16:11,726
You'll also hear new announcements
on TF-Agents for reinforcement learning,

308
00:16:11,726 --> 00:16:14,584
and tomorrow, we'll be discussing
the new TF federated library

309
00:16:14,584 --> 00:16:18,059
for federated learning.

310
00:16:18,059 --> 00:16:21,126
Deep learning research is also being
applied to real-world applications

311
00:16:21,126 --> 00:16:22,526
using TensorFlow.

312
00:16:22,526 --> 00:16:25,118
Here are a few examples
from researchers at Google

313
00:16:25,118 --> 00:16:27,817
where we see them applying it
to areas like our data centers.

314
00:16:27,817 --> 00:16:30,311
We're making them more efficient
with AI control system

315
00:16:30,311 --> 00:16:32,332
that delivers energy savings.

316
00:16:32,332 --> 00:16:34,923
Our apps like Google Maps,
the one shown in the middle,

317
00:16:34,923 --> 00:16:38,270
which has a new navigation feature
called global localization.

318
00:16:38,270 --> 00:16:42,072
It combines visual positioning service, 
street view, and machine learning

319
00:16:42,072 --> 00:16:45,248
to more accurately identify
position and orientation.

320
00:16:45,248 --> 00:16:47,228
And devices like the Google Pixel

321
00:16:47,228 --> 00:16:49,777
that use machine learning
to improve depth estimation

322
00:16:49,777 --> 00:16:51,572
to create better portrait mode photos

323
00:16:51,572 --> 00:16:54,679
like the one shown here.

324
00:16:54,679 --> 00:16:57,251
In order to make these
real-world applications a reality,

325
00:16:57,251 --> 00:17:00,293
you must be able to take models
from research and prototyping.

326
00:17:00,293 --> 00:17:02,273
all the way through
to launching and production.

327
00:17:02,273 --> 00:17:05,601
This has always been a core strength
and focus for TensorFlow.

328
00:17:05,601 --> 00:17:07,601
Using TensorFlow,
you can deploy your models

329
00:17:07,601 --> 00:17:10,037
on a number of platforms
like shown here.

330
00:17:10,037 --> 00:17:11,728
And models end up in a lot of places,

331
00:17:11,728 --> 00:17:13,688
so we want to make sure
TensorFlow works well

332
00:17:13,688 --> 00:17:16,483
across all of these
on servers and in cloud,

333
00:17:16,483 --> 00:17:18,491
on mobile and other Edge devices,

334
00:17:18,491 --> 00:17:20,675
in browser and JavaScript platforms.

335
00:17:20,675 --> 00:17:22,382
We have products for each of these:

336
00:17:22,382 --> 00:17:25,644
TensorFlow Extended,
TensorFlow Lite and TensorFlow.js

337
00:17:25,644 --> 00:17:28,931
which I'll briefly talk through.

338
00:17:28,931 --> 00:17:31,256
TensorFlow Extended
is our end-to-end platform

339
00:17:31,256 --> 00:17:34,078
for managing every stage
of the machine learning lifecycle.

340
00:17:34,078 --> 00:17:37,233
This spans all the way from ingesting
and transforming your data

341
00:17:37,233 --> 00:17:39,919
to deploying your
machine learning models at scale.

342
00:17:39,919 --> 00:17:44,245
In orange shown here, you can see
the libraries of open-sourced so far.

343
00:17:44,245 --> 00:17:45,556
What this slide alludes to is

344
00:17:45,556 --> 00:17:47,079
that we're now taking a step further

345
00:17:47,079 --> 00:17:49,309
and providing components
built from these libraries

346
00:17:49,309 --> 00:17:51,801
that make up an end-to-end platform.

347
00:17:51,801 --> 00:17:53,333
And note these
are the same components

348
00:17:53,333 --> 00:17:54,587
that are used internally

349
00:17:54,587 --> 00:17:56,119
in thousands of production systems,

350
00:17:56,119 --> 00:17:59,638
powering Google's
most important products.

351
00:17:59,638 --> 00:18:01,739
The components are only part of the story.

352
00:18:01,739 --> 00:18:04,082
2019 is the year
we're putting it all together,

353
00:18:04,082 --> 00:18:07,612
and providing you with an integrated 
end-to-end platform.

354
00:18:07,612 --> 00:18:09,766
First, you can bring
your own orchestrator.

355
00:18:09,766 --> 00:18:13,299
Here, we're showing airflow
or kubeflow, even raw kubernetes,

356
00:18:13,299 --> 00:18:14,659
whatever you want.

357
00:18:14,659 --> 00:18:16,474
No matter what orchestrator you choose,

358
00:18:16,474 --> 00:18:19,943
the TensorFlow extending components
integrate with a meta-data store.

359
00:18:19,943 --> 00:18:22,287
This store keeps track of all
the component runs,

360
00:18:22,287 --> 00:18:24,622
the artifacts that went into them

361
00:18:24,622 --> 00:18:26,810
and the artifacts
that were also produced.

362
00:18:26,810 --> 00:18:30,091
This enables advanced features
like experiments, experimentation

363
00:18:30,091 --> 00:18:32,688
and experiment tracking,
model comparison

364
00:18:32,688 --> 00:18:33,982
and things along those lines

365
00:18:33,982 --> 00:18:35,565
that I'm sure you'll be excited about

366
00:18:35,565 --> 00:18:37,764
and will help you
as you iterate through

367
00:18:37,764 --> 00:18:40,102
and work with your production systems.

368
00:18:40,102 --> 00:18:43,126
We have an end-to-end talk coming up
later today from Clemens and his team

369
00:18:43,126 --> 00:18:44,972
in which they'll take you
on a complete tour

370
00:18:44,972 --> 00:18:49,390
of using TensorFlow Extended
to solve a real problem.

371
00:18:49,390 --> 00:18:52,355
Moving on, TensorFlow Lite is
our solution for running models

372
00:18:52,355 --> 00:18:54,064
on mobile and IoT hardware.

373
00:18:54,064 --> 00:18:57,898
it uses a custom streamline file
format and a stripped-down runtime,

374
00:18:57,898 --> 00:19:01,587
so you can deploy TensorFlow
models everywhere your users are.

375
00:19:01,587 --> 00:19:05,278
On-device models can be more
responsive to input than cloud backends,

376
00:19:05,278 --> 00:19:07,719
and they keep user data
on device for privacy

377
00:19:07,719 --> 00:19:10,336
which is very important,
especially in this day and age.

378
00:19:10,336 --> 00:19:13,898
Google and our partners
like IGE in China use TF Lite

379
00:19:13,898 --> 00:19:15,328
for all kinds of tools,

380
00:19:15,328 --> 00:19:18,707
including predictive text generation,
video segmentation

381
00:19:18,707 --> 00:19:21,805
and things like Edge detection.

382
00:19:21,805 --> 00:19:24,654
But under the hood,
TensorFlow Lite is about performance.

383
00:19:24,654 --> 00:19:28,175
You can deploy models
to CPU, GPU and even Edge TPUs,

384
00:19:28,175 --> 00:19:29,918
and expect fast performance,

385
00:19:29,918 --> 00:19:33,433
and we've been refining since we
launched TensorFlow Lite last year.

386
00:19:33,433 --> 00:19:36,417
By using the latest quantization
techniques on CPU,

387
00:19:36,417 --> 00:19:40,192
adding support for OpenGL 3.1
and Metal on GPUs,

388
00:19:40,192 --> 00:19:42,633
and tuning our performance
on Edge TPUs,

389
00:19:42,633 --> 00:19:46,327
we're constantly pushing the limits
of what is possible on device,

390
00:19:46,327 --> 00:19:50,199
and you should can expect even
greater enhancements in the year ahead.

391
00:19:50,199 --> 00:19:52,843
We'll hear details from Raziel
and his colleagues coming up

392
00:19:52,843 --> 00:19:56,186
in a little bit this morning.

393
00:19:56,186 --> 00:19:58,901
Javascript is the number one programming
language in the world

394
00:19:58,901 --> 00:20:01,282
and until recently hasn't
necessarily benefited

395
00:20:01,282 --> 00:20:03,851
from all the machine learning
development and tools.

396
00:20:03,851 --> 00:20:06,271
Last year we released TensorFlow.js,

397
00:20:06,271 --> 00:20:08,967
a library for training
and deploying machine learning models

398
00:20:08,967 --> 00:20:11,392
in the browser and on Node.js.

399
00:20:11,392 --> 00:20:14,351
Since then we've seen huge adoption
in the JavaScript community

400
00:20:14,351 --> 00:20:18,111
with more than 300,000 downloads
and 100 contributors,

401
00:20:18,111 --> 00:20:19,404
but we're just at the beginning

402
00:20:19,404 --> 00:20:23,562
given how big the JavaScript
and web ecosystem is.

403
00:20:23,562 --> 00:20:27,229
Today we're excited to announce 
TensorFlow.js version 1.0.

404
00:20:27,229 --> 00:20:29,932
This comes with many
improvements and new features.

405
00:20:29,932 --> 00:20:34,064
We have a library of off-the-shelf models
for common machine learning problems

406
00:20:34,064 --> 00:20:36,675
that run both in the
browser and on node.

407
00:20:36,675 --> 00:20:38,922
We're also adding support
for more platforms

408
00:20:38,922 --> 00:20:42,047
where JavaScript runs
such as electron desktop apps

409
00:20:42,047 --> 00:20:44,359
or mobile native platforms.

410
00:20:44,359 --> 00:20:47,590
and a huge focus in TensorFlow.js 1.0

411
00:20:47,590 --> 00:20:49,302
is on performance improvements.

412
00:20:49,302 --> 00:20:51,434
As an example, compared to last year,

413
00:20:51,434 --> 00:20:55,148
MobileNet inference and
browser is now nine times faster.

414
00:20:55,148 --> 00:20:59,870
You'll learn more about these advances
in our talk later in the day.

415
00:20:59,870 --> 00:21:02,794
Another language that we're
really excited about is swift.

416
00:21:02,794 --> 00:21:04,747
Swift for TensorFlow is reexamining

417
00:21:04,747 --> 00:21:07,290
what it means for
performance and usability.

418
00:21:07,290 --> 00:21:10,020
With a new stack built
on top of TensorFlow's core

419
00:21:10,020 --> 00:21:11,242
and a new programming model

420
00:21:11,242 --> 00:21:14,829
that intends to bring further usability.

421
00:21:14,829 --> 00:21:19,051
And today, we're announcing that Swift
for TensorFlow is now at version 0.2.

422
00:21:19,051 --> 00:21:21,596
It's ready for you to
experiment with, to try out,

423
00:21:21,596 --> 00:21:25,868
and we're really excited to be
bringing this to the community.

424
00:21:25,868 --> 00:21:28,416
In addition to telling you
about version 0.2,

425
00:21:28,416 --> 00:21:31,687
we're also excited to announce
that Jeremy Howard, a fast.ai

426
00:21:31,687 --> 00:21:34,480
is writing a new course
in Swift for TensorFlow.

427
00:21:34,480 --> 00:21:39,534
Chris and Brennan will tell you
a lot more about this later today.

428
00:21:39,534 --> 00:21:41,875
So to recap everything
we've shown you so far.

429
00:21:41,875 --> 00:21:44,281
TensorFlow has
grown to a full ecosystem

430
00:21:44,281 --> 00:21:45,732
from research to production,

431
00:21:45,732 --> 00:21:48,825
from server to mobile
with many languages.

432
00:21:48,825 --> 00:21:50,793
This growth has been
fueled by our community,

433
00:21:50,793 --> 00:21:53,720
and honestly would not
have been possible without the community.

434
00:21:53,720 --> 00:21:57,190
To talk about what we're planning
for you and with you in 2019,

435
00:21:57,190 --> 00:21:59,846
I'll hand it over to Kemal.

436
00:21:59,846 --> 00:22:03,684
(applause)

437
00:22:03,684 --> 00:22:05,510
It's all you.

438
00:22:05,510 --> 00:22:07,286
(Kemal) Thank you, Megan.

439
00:22:07,286 --> 00:22:08,818
Hi, my name is Kemal

440
00:22:08,818 --> 00:22:11,733
and I'm the Product Director
for TensorFlow.

441
00:22:11,733 --> 00:22:16,216
I'm really excited to be here today 
for this celebration,

442
00:22:16,216 --> 00:22:18,629
and what we're celebrating
is the most important part

443
00:22:18,629 --> 00:22:21,627
of what we're building,
and that's the community.

444
00:22:21,627 --> 00:22:24,334
Personally, I love building 
developer platforms.

445
00:22:24,334 --> 00:22:26,826
I used to be a developer
as an entrepreneur,

446
00:22:26,826 --> 00:22:29,168
and now I get to
enable other developers

447
00:22:29,168 --> 00:22:32,810
by building together a better platform.

448
00:22:32,810 --> 00:22:34,477
When we started working on 2.0,

449
00:22:34,477 --> 00:22:36,574
we turned to the community,

450
00:22:36,574 --> 00:22:38,993
we started with the request
for common process,

451
00:22:38,993 --> 00:22:42,749
consulting with all of you
on important product decisions.

452
00:22:42,749 --> 00:22:44,521
We received valuable feedback

453
00:22:44,521 --> 00:22:48,324
and we couldn't have built 2.0 
without you.

454
00:22:48,324 --> 00:22:50,476
And some of you wanted
to get more involved

455
00:22:50,476 --> 00:22:53,501
so we created special
interest groups or sigs

456
00:22:53,501 --> 00:22:56,717
like Networking
or Tensor Board to name a few.

457
00:22:56,717 --> 00:22:58,940
And sigs are really
a great way for the community

458
00:22:58,940 --> 00:23:00,750
to build the pieces of TensorFlow

459
00:23:00,750 --> 00:23:04,014
that they care the most about.

460
00:23:04,014 --> 00:23:06,229
We also wanted to hear more
about what you were building,

461
00:23:06,229 --> 00:23:08,906
so we launched a Powered
By TensorFlow campaign.

462
00:23:08,906 --> 00:23:12,669
And I am going to say we were amazed
by the creativity of the project,

463
00:23:12,669 --> 00:23:15,940
from biological image analysis,
to custom wearables,

464
00:23:15,940 --> 00:23:18,544
to chat BOTS.

465
00:23:18,544 --> 00:23:22,101
So after three years,
our community is really thriving.

466
00:23:22,101 --> 00:23:26,057
There're almost 70
machine learning GTEs right now.

467
00:23:26,057 --> 00:23:27,198
Around the world,

468
00:23:27,198 --> 00:23:30,092
1800 contributors on core alone,

469
00:23:30,092 --> 00:23:33,000
and countless more of you
who are doing amazing work

470
00:23:33,000 --> 00:23:35,161
to help make TensorFlow successful.

471
00:23:35,161 --> 00:23:37,543
So on behalf of the whole TensorFlow team

472
00:23:37,543 --> 00:23:40,558
we want to say a huge thank you.

473
00:23:40,558 --> 00:23:45,740
(applause)

474
00:23:45,740 --> 00:23:47,884
So we have big plans for 2019,

475
00:23:47,884 --> 00:23:51,336
and I would like to make
a few announcements.

476
00:23:51,336 --> 00:23:53,582
First, as our community grows,

477
00:23:53,582 --> 00:23:56,217
we welcome people
who are new to machine learning

478
00:23:56,217 --> 00:23:57,900
and it's really important
to provide them

479
00:23:57,900 --> 00:24:00,675
with the best educational material,

480
00:24:00,675 --> 00:24:03,700
so we're excited to announce

481
00:24:03,700 --> 00:24:05,880
two new online courses.

482
00:24:05,880 --> 00:24:07,793
One is with deeplearning.ai

483
00:24:07,793 --> 00:24:10,201
and it's published
in the Coursera platform.

484
00:24:10,201 --> 00:24:12,510
And the other's with Udacity.

485
00:24:12,510 --> 00:24:16,590
The first batch of these lessons
is available right now,

486
00:24:16,590 --> 00:24:20,653
and they provide an awesome introduction
to TensorFlow for developers.

487
00:24:20,653 --> 00:24:23,558
They require no prior knowledge
to machine learning,

488
00:24:23,558 --> 00:24:27,134
so I highly encourage you
to check them out.

489
00:24:27,134 --> 00:24:30,651
Next, if your students
for the very first time,

490
00:24:30,651 --> 00:24:33,571
you can apply to the
Google Summer of Code program

491
00:24:33,571 --> 00:24:36,332
and get to work
with the TensorFlow engineering team

492
00:24:36,332 --> 00:24:40,788
to help build a part of TensorFlow.

493
00:24:40,788 --> 00:24:44,096
I also talked about the Powered
By TensorFlow campaign.

494
00:24:44,096 --> 00:24:46,715
We're so excited with the creativity

495
00:24:46,715 --> 00:24:50,839
that we decided to launch
a 2.0 hackathon on DevPost

496
00:24:50,839 --> 00:24:53,039
post to let you share
your latest and greatest,

497
00:24:53,039 --> 00:24:55,053
and win cool prizes.

498
00:24:55,053 --> 00:24:59,934
So we're really excited to see
what you're going to build.

499
00:24:59,934 --> 00:25:02,815
Finally, as our ecosystem grows,

500
00:25:02,815 --> 00:25:04,652
we're now having
a second day at the summit,

501
00:25:04,652 --> 00:25:08,229
but we really wanted
to do something more.

502
00:25:08,229 --> 00:25:10,139
We wanted a place where you can share

503
00:25:10,139 --> 00:25:12,486
what you've been building on TensorFlow,

504
00:25:12,486 --> 00:25:15,586
so we're excited to announce

505
00:25:15,586 --> 00:25:17,286
TensorFlow World,

506
00:25:17,286 --> 00:25:21,876
a week-long conference dedicated
to open-source collaboration.

507
00:25:21,876 --> 00:25:25,969
This conference will be co-presented
by O'Reilly Media and TensorFlow,

508
00:25:25,969 --> 00:25:29,126
and will be held
in Santa Clara end of October.

509
00:25:29,126 --> 00:25:32,936
Our vision is to bring together
the awesome TensorFlow World

510
00:25:32,936 --> 00:25:36,611
and give a place for folks
to connect with each other.

511
00:25:36,611 --> 00:25:39,469
So I'd like to invite on stage Gina Blaber

512
00:25:39,469 --> 00:25:42,653
to say a few words about the conference.

513
00:25:42,653 --> 00:25:48,065
(applause)

514
00:25:48,065 --> 00:25:50,537
(Gina) Thank You, Kemal.

515
00:25:50,537 --> 00:25:52,252
O'Reilly is a learning company

516
00:25:52,252 --> 00:25:55,330
with a focus
on technology and business.

517
00:25:55,330 --> 00:25:58,179
We have strong ties
with the open source community

518
00:25:58,179 --> 00:25:59,429
as many of you know,

519
00:25:59,429 --> 00:26:03,555
and we have a history
of bringing big ideas to life.

520
00:26:03,555 --> 00:26:06,865
That's why we're excited
about partnering with TensorFlow

521
00:26:06,865 --> 00:26:08,089
to create this new event

522
00:26:08,089 --> 00:26:12,800
that brings machine learning
and AI to the community.

523
00:26:12,800 --> 00:26:16,499
The event of TensorFlow
happening on October 28 to 31

524
00:26:16,499 --> 00:26:17,989
in Santa Clara.

525
00:26:17,989 --> 00:26:21,172
And when I say community,
I mean everyone.

526
00:26:21,172 --> 00:26:24,445
We want to bring together 
the entire TensorFlow community

527
00:26:24,445 --> 00:26:26,630
of individuals and teams,

528
00:26:26,630 --> 00:26:28,231
and enterprises.

529
00:26:28,231 --> 00:26:30,135
This is the place 
where you'll meet experts

530
00:26:30,135 --> 00:26:31,430
from around the world,

531
00:26:31,430 --> 00:26:33,808
the team that actually
creates TensorFlow,

532
00:26:33,808 --> 00:26:38,985
and the companies and enterprises
that will help you deploy it.

533
00:26:38,985 --> 00:26:43,339
We have an open CFP right now
on the TensorFlow World site.

534
00:26:43,339 --> 00:26:47,131
I invite you all to check that out
and send in your proposal soon,

535
00:26:47,131 --> 00:26:49,342
so your voice is heard at that event.

536
00:26:49,342 --> 00:26:52,520
We look forward to seeing you 
at TensorFlow World in October.

537
00:26:52,520 --> 00:26:54,175
Thank you.

538
00:26:54,175 --> 00:26:58,452
(applause)

539
00:26:58,452 --> 00:27:00,153
Thank you, Gina. 
This is going to be great.

540
00:27:00,153 --> 00:27:02,401
Are you guys excited?

541
00:27:02,401 --> 00:27:05,026
Woo!

542
00:27:05,026 --> 00:27:07,314
So we have a few calls to action for you.

543
00:27:07,314 --> 00:27:12,136
Take a course, submit a talk 
to TF World, start hacking in 2.0.

544
00:27:12,136 --> 00:27:15,303
By the way, the grand prizes 
for a hackathon on DevPost,

545
00:27:15,303 --> 00:27:19,832
will include free tickets
to TensorFlow World.

546
00:27:19,832 --> 00:27:23,641
You know one thing that I love is 
to hear about these amazing stories

547
00:27:23,641 --> 00:27:26,978
of people building awesome stuff 
on top of TensorFlow.

548
00:27:26,978 --> 00:27:30,383
And as a team, we really
believe that AI advances faster

549
00:27:30,383 --> 00:27:32,626
when people have access to our tools

550
00:27:32,626 --> 00:27:35,871
and can then apply them 
to the problems that they care about

551
00:27:35,871 --> 00:27:38,592
in ways that we never really dreamed of.

552
00:27:38,592 --> 00:27:43,345
And when people can really do that,
some special things happen.

553
00:27:43,345 --> 00:27:47,490
And I'd like to share 
with you something really special.

554
00:27:47,490 --> 00:27:51,511
(urban noise)

555
00:27:51,511 --> 00:27:53,169
Looking at historical documents

556
00:27:53,169 --> 00:27:55,543
and especially documents 
from the Middle Age period,

557
00:27:55,543 --> 00:27:59,015
requires a lot of time 
and also a lot of patience.

558
00:27:59,015 --> 00:28:01,893
♪ (gentle music) ♪

559
00:28:01,893 --> 00:28:07,128
In the Vatican Archives,
there are 85km of documents

560
00:28:07,128 --> 00:28:09,874
more or less the length
of the Panama Canal.

561
00:28:09,874 --> 00:28:12,197
The scriptures written 
in the medieval handwriting

562
00:28:12,197 --> 00:28:14,555
are different from the ones 
we know nowadays.

563
00:28:14,555 --> 00:28:18,024
If one day someone ask me
to transcribe and translate

564
00:28:18,024 --> 00:28:20,811
all the documents 
of the Vatican Archive,

565
00:28:20,811 --> 00:28:23,053
I would tell them 
that they are completely crazy.

566
00:28:23,053 --> 00:28:25,220
(woman) Looking at this book page by page

567
00:28:25,220 --> 00:28:28,773
and trying to decipher,
read and transcribe whatever is there

568
00:28:28,773 --> 00:28:32,376
takes an enormous amount of time.

569
00:28:32,376 --> 00:28:34,959
It would require an army
of paleographer.

570
00:28:34,959 --> 00:28:36,505
♪ (upbeat music) ♪

571
00:28:36,505 --> 00:28:39,690
(woman 2) What I am excited 
the most about machine learning is

572
00:28:39,690 --> 00:28:42,086
that it enabled us to solve problems

573
00:28:42,086 --> 00:28:45,911
that up to 10, 15 years ago 
we thought unsolvable.

574
00:28:45,911 --> 00:28:49,316
(Paolo) "In Codice Ratio" was born 
from this idea of building a software

575
00:28:49,316 --> 00:28:52,980
that can read and interpret 
what is inside those manuscripts.

576
00:28:52,980 --> 00:28:55,689
When we started discussing the problem,

577
00:28:55,689 --> 00:28:58,030
we realized that a solution 
based on neural networks

578
00:28:58,030 --> 00:29:00,442
was absolutely necessary.

579
00:29:00,442 --> 00:29:04,610
The choice of TensorFlow
was a natural one.

580
00:29:04,610 --> 00:29:08,055
(Elena) Before using any kind 
of machine learning module,

581
00:29:08,055 --> 00:29:09,627
we needed to collect data first.

582
00:29:09,627 --> 00:29:13,287
You have thousands of images 
of dogs and cats on the Internet,

583
00:29:13,287 --> 00:29:17,301
but there's very little images 
of ancient manuscripts.

584
00:29:17,301 --> 00:29:21,570
We build our own custom 
web application for crowd sourcing

585
00:29:21,570 --> 00:29:26,828
and we involved high school students 
to collect the data.

586
00:29:26,828 --> 00:29:29,336
I didn't know much 
about machine learning in general,

587
00:29:29,336 --> 00:29:34,283
but I found it very easy 
to create a TensorFlow environment.

588
00:29:34,283 --> 00:29:38,260
When we were trying to figure out
which model worked best for us,

589
00:29:38,260 --> 00:29:39,936
Keras was the best solution.

590
00:29:39,936 --> 00:29:44,604
The production model runs on 
TensorFlow layers and estimator interface.

591
00:29:44,604 --> 00:29:47,515
We experimented 
with binary classification

592
00:29:47,515 --> 00:29:49,285
with fully connected networks,

593
00:29:49,285 --> 00:29:52,646
and finally we move 
to convolutional neural network

594
00:29:52,646 --> 00:29:54,497
and multi-class classification.

595
00:29:54,497 --> 00:30:00,052
In a short time, we were able to develop 
and test the first solutions.

596
00:30:00,052 --> 00:30:02,664
When it comes to recognizing 
single characters,

597
00:30:02,664 --> 00:30:06,121
we can get 95% average accuracy.

598
00:30:06,121 --> 00:30:11,276
(Marco) Being able to access an IT tool
greatly shortens the timing.

599
00:30:11,276 --> 00:30:14,768
Being able to solve certain abbreviations 
and to understand a text

600
00:30:14,768 --> 00:30:17,427
in that cryptic writing

601
00:30:17,427 --> 00:30:19,268
is something exceptional.

602
00:30:19,268 --> 00:30:22,810
(Serena) This will have an enormous
impact in a short period of time.

603
00:30:22,810 --> 00:30:27,110
We will have a massive quantity 
of historical information available.

604
00:30:27,110 --> 00:30:29,927
I just think solving problems is fun.

605
00:30:29,927 --> 00:30:32,167
It's a game against myself,

606
00:30:32,167 --> 00:30:34,277
and how good I can do.

607
00:30:34,277 --> 00:30:37,329
(Marco) The study of history 
is extremely important

608
00:30:37,329 --> 00:30:39,702
to understand our present

609
00:30:39,702 --> 00:30:46,997
and to get a perspective on the future.

610
00:30:46,997 --> 00:30:51,705
(applause)

611
00:30:51,705 --> 00:30:53,563
This is such a great story.

612
00:30:53,563 --> 00:30:57,500
I think about the scholars
who wrote these manuscripts.

613
00:30:57,500 --> 00:31:00,366
They couldn't have been
imagined that centuries later,

614
00:31:00,366 --> 00:31:03,270
people will be using computers 
to bring back to life their work.

615
00:31:03,270 --> 00:31:05,778
So we're really lucky 
to have Elena with us today.

616
00:31:05,778 --> 00:31:08,755
Elena, would you stand?

617
00:31:08,755 --> 00:31:16,060
(applause)

618
00:31:16,060 --> 00:31:19,276
Don't miss the talk 
where she will share her story today.

619
00:31:19,276 --> 00:31:20,764
I really hope you have a great day.

620
00:31:20,764 --> 00:31:22,551
We have some really awesome things.

621
00:31:22,551 --> 00:31:24,969
The team and I will be around.

622
00:31:24,969 --> 00:31:27,536
Please come and say hi,
we want to hear from you,

623
00:31:27,536 --> 00:31:30,474
and with that, I'm going 
to hand it over to Martin

624
00:31:30,474 --> 00:31:32,625
who will talk about TensorFlow 2.0.
Thank you.

625
00:31:32,625 --> 00:31:36,982
♪ (upbeat music) ♪

